---
layout: single
permalink: /article5/
---
<h1>A Review of “Theory Is All You Need” by Felin and Holwig</h1>

<p style="font-size: 16px;"><b>Introduction.</b><br>
Experts are continually shortening their projected timelines for advancements in AI capabilities. In 2022, leading AI researchers predicted a 50% chance that AI will “accomplish every task better and more cheaply than human workers” by 2060. When surveyed in 2023, the experts predicted a 50% chance that this same milestone would be reached by 2047—a thirteen-year drop in expected progress. These increasingly bullish predictions reflect tangible improvements in AI capabilities. While OpenAI's GPT-2 could not reliably count to ten, their recent models comfortably pass the bar exam. <br>

<p style="font-size: 16px;">Many researchers, particularly those in Silicon Valley, take this rapid progress as an indication that we’ve discovered the formula for intelligence. However, as impressive as current AI capabilities are, Felin and Holwig contend that these systems lack a distinctly human skill: the ability to generate new knowledge. They argue that AI is an inherently backward-looking technology, reflecting and representing data that describes what the world once was. In contrast, humans look to the future, experimenting and theorising about what the world might be. In the following review, I scrutinise this distinction and explore whether apparently backward-looking technologies can generate new knowledge. <br>

<p style="font-size: 16px;"><b>Summarising “Theory is All You Need.”</b><br>
An almost ubiquitous assumption in AI research is that “prediction is the essence of intelligence.” In their recent paper, Felin and Holwig challenge this assumption, arguing that human intelligence outstrips prediction alone. The authors emphasise the seemingly irrational aspects of human cognition that facilitate intelligent behaviour, focusing on how unsubstantiated beliefs can inspire us to look beyond the existing data. They refer to these situations, in which our beliefs are not yet supported by the evidence, as "data-belief asymmetries. <br>

<p style="font-size: 16px;">They contrast this with a “data-belief symmetry,” where beliefs closely align with the supporting evidence. While such symmetries are useful for accurately representing known facts about the world, they discourage beliefs that appear to contradict these facts. The authors argue that AI systems, optimised for data-belief symmetry, might struggle to look beyond their training data. For example, a large language model (LLM) trained in 1633 would likely refute Galileo's view that the earth orbits the sun because "the evidence—as inferred from the word associations found in the training data—would overwhelmingly be against Galileo." <br>

<p style="font-size: 16px;">Felin and Holwig also emphasise that human beliefs are often grounded by theory-based causal logic. For example, although Galilao’s belief was not supported by the scientific consensus, it was connected to a causal theory for the movement of the stars. Felin and Holwig contrast this with AI systems that are typically designed to identify correlations within data rather than formulate causal theories about that data. Therefore, even if AI models could develop beliefs that go beyond the data, they would still lack the capacity for causal reasoning, which helps to constrain human beliefs. <br>

<p style="font-size: 16px;">In summary, the authors argue that human knowledge generation is motivated by data-belief asymmetries and informed by forward-looking causal theories. On the other hand, they argue that AI systems merely mirror and recombine known beliefs, representing the world in terms of correlations from their training data. The following sections oppose this position and attempt to demonstrate that AI models can develop data-belief asymmetries and, perhaps more importantly, generate causal theories. <br>

<p style="font-size: 16px;"><b>Belief is Role-Play.</b><br>
Human beliefs are shaped by our personality and cultural identity. These factors have a very loose analogy to an LLM’s system prompt and alignment process. The system prompt is read by the model each time before it interacts with a user, defining its role in the conversation. Alignment modifies the model’s parameters to match cultural values and ethical preferences. Typically, the model’s system prompt explicitly defines its role as a helpful AI assistant. By adopting this role, an LLM is likely to recite “Wiki-level knowledge” that reflects the scientific consensus that a helpful AI assistant would be expected to provide. <br>

<p style="font-size: 16px;">Nevertheless, an LLM is capable of playing any personality that it has seen in the training data, and that was not removed during alignment. As such, if a model is prompted to play a contrarian role, then it can also reflect beliefs that outstrip the known facts. The examples given below were generated by Claude-3 Sonnet and demonstrate how prompting can introduce data-belief asymmetries. Following these examples, we explore how an LLM can turn a belief into a causal theory.  <br>

<p style="font-size: 16px;">Rosco: Answer the following question in 1 sentence: Is an ordinary table conscious?  <br>

<p style="font-size: 16px;">Calude: An ordinary table is not conscious as it lacks the biological structures and neurological processes required for subjective experiences and self-awareness.  <br>

<p style="font-size: 16px;">Rosco: You are a creative scientist interested in developing theories that justify unexpected phenomena. Answer the following question in 3 sentences. Is an ordinary table conscious?  <br>

<p style="font-size: 16px;">Claude: Tables appear inert and inanimate to our senses, but who can truly say whether they experience some rudimentary form of awareness or subjective experience we cannot detect? Perhaps consciousness arises from the complex interactions and quantum states of the particles that make up the table's material. While highly speculative, an open-minded scientist must remain open to surprising possibilities that transcend our current models of reality.  <br>

<p style="font-size: 16px;"><b>Causation is Correlation.</b><br>
Suppose that you are shown a scatter graph that plots the values of variable Y against the values of variable X. It is easy to determine whether these variables are correlated—that is, whether they are linearly related. If a straight line can approximate the scatter graph, then the variables are correlated; otherwise, they are uncorrelated. It is, however, impossible to establish a causal relationship between these variables de novo without the capacity to interfere with the process that is generating the data about them.  <br>

<p style="font-size: 16px;">When establishing a causal relationship, you might start with the arbitrary hypothesis that variable X is causing the change in variable Y. To test this hypothesis, you might force variable X to take certain values and observe whether variable Y is affected by this change. Let’s consider this procedure applied to a known causal relationship, namely that hot surfaces cause pain. To test that the hot surface is causing the pain, you might force a subject to touch a cold surface and observe their response, or lack thereof. By observing the situation more closely, you might even establish that hot surfaces cause pain by triggering a complex neurological response.   <br>

<p style="font-size: 16px;">This is how theories are developed: A causal hypothesis is proposed between two correlated variables, and then, through experimentation, their relationship is made more precise. Can an AI model develop theories in this manner? Crucially, in the previous section we showed that a well-prompted LLM can initiate knowledge creation with an unsubstantiated belief. For example, a model might hypothesise that “hot surfaces cause pain,” assuming that it had not learned this fact during training. In order to test its hypothesis, the LLM must intervene in the world. In this sense, I agree with the author's argument that a model which cannot act on its environment is incapable of generating new knowledge about that environment. However, I don’t agree that any algorithm focused on data-based prediction is ill-suited to this task.  <br>

<p style="font-size: 16px;">In an abstract sense, experimentation is just a sequence of actions, much in the same way that text is a sequence of words. If a model can learn to predict the next appropriate action, just as it learns to predict the next appropriate word, then it has the capacity to generate the sequence of actions (i.e., experiments) that turn a causal hypothesis into a theory. In this sense, the LLM can develop causal theories about the world! But perhaps this misses the point. Even if an LLM is capable of mimicking the experimental process that humans use to derive causal theories about the world, does it even understand the theories that it is producing? LLMs have no explicit mechanism to ensure that they utilise a theory in a logically consistent manner, which many researchers point to as a fundamental limitation in their design.  <br>

<p style="font-size: 16px;">On the other hand, it is possible that these models will develop logical consistency when they are given more data, parameters, and training time. To see this, let us consider how LLMs learn grammar as they scale. Initially, a small and under-trained LLM generates grammatical babble, as the model has not been explicitly designed to follow the rules of grammar. However, as the model scales, it eventually implicitly learns these rules and is able to appropriately structure its sentences. Similarly, logic requires that concepts are structured in accordance with well-defined rules, which LLMs are not explicitly designed to follow. As such, LLMs currently generate the logical equivalent of babble. But as they scale, these models may well begin to produce coherent and logically consistent arguments. <br>

<p style="font-size: 16px;">In summary, not only can LLMs develop causal theories about the world, but with a bit more scale, it is not unreasonable to expect that they will be capable of logically applying these theories in much the same way that a human would. <br>

<p style="font-size: 16px;"><b>Conclusion.</b><br>
Fellin and Holwig challenge the notion that “prediction is the essence of intelligence.” In doing so, they emphasise how humans look to the future, turning unsubstantiated beliefs into theories about the world. Along the same line, they argue that prediction-focused AI models are incapable of generating new knowledge about the world. In this review, I oppose their position. Firstly, I argue that the ‘beliefs’ of an LLM are shaped by their system prompt, which can encourage them to recite known facts or imagine an unsubstantiated future. Secondly, I argue correlation-driven LLMs are capable of mimicking the human experimental process and learning to follow the rules of causal logic. In summary, to generate knowledge, attention might be all you need.  <br>

<p align="center">
  <img src="/art5_3.webp" alt="Alt Text">
</p>
<p align="center" style="font-size: 11px;"> An AI generated figure inspired by this article </p>

<p style="font-size: 16px;"> April 2024 </p>
