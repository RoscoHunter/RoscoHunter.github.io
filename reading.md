---
layout: single
permalink: /reading/
---
<h1>Works That Shaped My Thinking</h1>

<p style="font-size: 16px;"><b>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.</b><br>
<em>Frankle, Jonathan, and Michael Carbin. ICLR, 2021.</em><br>

<p style="font-size: 16px;"><b>Neural Tangent Kernel: Convergence and Generalization in Neural Networks.</b><br>
<em>Jacot, Arthur, Franck Gabriel, and Cl√©ment Hongler. NeurIPS, 2018.</em><br>
  
<p style="font-size: 16px;"><b>Attention Approximates Sparse Distributed Memory.</b><br>
<em>Bricken, Trenton, and Cengiz Pehlevan. NeurIPS, 2021.</em><br>

<p style="font-size: 16px;"><b>Constitutional AI: Harmlessness from AI Feedback.</b><br>
<em>Bai, Yuntao, et al. Anthropic, 2022.</em><br>

<p style="font-size: 16px;"><b>A Deeper Look at Zero-Cost Proxies for Lightweight NAS</b><br>
<em>White, Colin, et al. ICLR, 2022.</em><br>

<p style="font-size: 16px;"><b>Political Theory of the Digital Age: Where AI Might Take Us.</b><br>
<em>Risse, Mathias. Cambridge University Press, 2023.</em><br>

<p style="font-size: 16px;"><b>Automl-Zero: Evolving Machine Learning Algorithms from Scratch.</b><br>
<em>Real, Esteban, et al. ICML, 2020.</em><br>

<p style="font-size: 16px;"><b>Score-Based Generative Modelling Through Stochastic Differential Equations.</b><br>
<em>Song, Yang, et al. ICLR, 2021.</em><br>

<p style="font-size: 16px;"><b>Toy Models of Superposition.</b><br>
<em>Elhage, Nelson, et al. Anthropic, 2022.</em><br>

