---
layout: single
permalink: /reading/
---
<h1>Works That Shape My Research</h1>

<p style="font-size: 16px;"><b>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.</b><br>
<em>Frankle, Jonathan, and Michael Carbin.</em><br>
<em>International Conference on Learning Representations, 2021.</em><br>

<p style="font-size: 16px;"><b>Neural Tangent Kernel: Convergence and Generalization in Neural Networks.</b><br>
<em>Jacot, Arthur, Franck Gabriel, and Cl√©ment Hongler.</em><br>
<em>Advances in Neural Information Processing Systems, 2018.</em><br>
  
<p style="font-size: 16px;"><b>Attention Approximates Sparse Distributed Memory.</b><br>
<em>Bricken, Trenton, and Cengiz Pehlevan.</em><br>
<em>Advances in Neural Information Processing Systems, 2021.</em><br>

<p style="font-size: 16px;"><b>Zero-Cost Proxies for Lightweight NAS.</b><br>
<em>Abdelfattah, Mohamed S., et al.</em><br>
<em>International Conference on Learning Representations, 2019.</em><br>

<p style="font-size: 16px;"><b>Political Theory of the Digital Age: Where AI Might Take Us.</b><br>
<em>Risse, Mathias.</em><br>
<em>Cambridge University Press, 2023.</em><br>

<p style="font-size: 16px;"><b>Automl-zero: Evolving machine learning algorithms from scratch.</b><br>
<em>Real, Esteban, et al.</em><br>
<em>International Conference on Machine Learning, 2020.</em><br>

<p style="font-size: 16px;"><b>Constitutional AI: Harmlessness from AI Feedback.</b><br>
<em>Bai, Yuntao, et al.</em><br>
<em>Anthropic, 2022.</em><br>

<p style="font-size: 16px;"><b>Brain Computations: What and How.</b><br>
<em>Rolls, Edmund T.</em><br>
<em>Oxford University Press, 2021.</em><br>

<p style="font-size: 16px;"><b>Gilles Deleuze: An Introduction</b><br>
<em>May, Todd.</em><br>
<em>Cambridge University Press, 2011.</em><br>
